
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Visual Semantic Relatedness Dataset for Image Captioning</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="style.css" rel="stylesheet" type="text/css" />

<script type="text/javascript" src="../js/hidebib.js"></script>



</head>

<body> 

<div class="container">
  <table border="0" align="center">
    <tr><td width="623" align="center" valign="middle"><h2>Visual Semantic Relatedness Dataset for Image Captioning </h2></td></tr>
    <tr><td width="300" align="center" valign="middle">   <a href="https://www.cs.upc.edu/~asabir/">Ahmed Sabir</a>, <a href="https://www.iri.upc.edu/people/fmoreno/">Francesc Moreno-Noguer</a>, 
         <a href="https://www.cs.upc.edu/~padro/">Lluís Padró</a></td></tr>
  </table>
  </br>
  


  <p align="center"> <img src="overview.png" width="500" align="middle" /></p>



  


</table>
<table width="400px" align="center">
  <tr>
    <td align=center width=100px>
      <center>
        <ul class="nav">
          <li class="nav-item text-center">
              <a href="https://arxiv.org/" class="nav-link" title="Temp link">
                  <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                      <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                  </svg><br>
                  Paper
              </a>
      </center>
    </td>
    <td align=center width=200px>
      <center>
        
        <ul class="nav">
        <li class="nav-item text-center">
          <a href="https://github.com" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                  <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
              </svg> <br>
             
              Code
          </a>
      </center>
      <td align=center width=100px>
        <center>
          <ul class="nav">
          <li class="nav-item text-center">
            <a href="https://huggingface.co/datasets/AhmedSSabir/Textual-Image-Caption-Dataset" class="nav-link">
              <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                <path fill="currentColor" d="M21,16.5C21,16.88 20.79,17.21 20.47,17.38L12.57,21.82C12.41,21.94 12.21,22 12,22C11.79,22 11.59,21.94 11.43,21.82L3.53,17.38C3.21,17.21 3,16.88 3,16.5V7.5C3,7.12 3.21,6.79 3.53,6.62L11.43,2.18C11.59,2.06 11.79,2 12,2C12.21,2 12.41,2.06 12.57,2.18L20.47,6.62C20.79,6.79 21,7.12 21,7.5V16.5M12,4.15L6.04,7.5L12,10.85L17.96,7.5L12,4.15M5,15.91L11,19.29V12.58L5,9.21V15.91M19,15.91V9.21L13,12.58V19.29L19,15.91Z" />
              </svg><br>
              data
            </a>
                  </ul>
        </center>
    </td>
</table>

  
</div>

</br>

<div class="container">
  <h2>Abstract</h2>

  

  




    <p align="justify"> Modern image captaining relies heavily on extracting knowledge, from images such as objects, to capture the concept of static story inthe image. 
      In this paper, we propose a textual visual context dataset for captioning, where the publicly available dataset COCO caption (<a href="https://www.aaai.org/Papers/Symposia/Spring/2003/SS-03-05/SS03-05-005.pdf">Lin et al., 2014</a>) 
      has been extended with information about the scene (such as objects in the image). Since this information has textualform, it can be used to leverage any 
      NLP task, such as text similarity or semantic relation methods, into captioning systems, either as anend-to-end training strategy or a post-processing based approach.</a></tr>
</div>

</br>





<div class="container" text-align="left">
  <h2>Overview</h2>
    <table border="0" align="left">
      <tr><td width="1000" align="justify"> 
        <p align="justify"> We enrich COCO-caption with textual Visual Context information. We use ResNet152, CLIP and Faster R-CNN to extract object information for each COCO-caption image. We use three filter 
        approaches to ensure quality of the dataset (1) Threshold: to filter out predictions where the object classifier is not confident enough, and (2) semantic alignment to with semantic similarity 
        to remove duplicated object. (3) semantic relatedness score as soft-label: to grantee the visual context and caption have strong relation, we use Sentence RoBERTa-SBERT uses siamese network to 
        derive meaningfully sentence embedding that can be compared via cosine similarity to give a soft label via cosine similarity with threshold to annotate the final label (if th > 0.2, 0.3, 0.4 then 1,0). 
        Finally, to take advantage of the overlapping between the visual context and the caption, and to extract global information from each visual, we use BERT followed by a shallow CNN (Kim, 2014).
       </br>
  
        
    Here is a  <a href="https://colab.research.google.com/drive/1N0JVa6y8FKGLLSpiG7hd_W75UYhHRe2j?usp=sharing">Colab</a> demo </td></tr>
    
    
    
  
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_agent">
          <h2>Dataset</h2>
          <p>   We proposed a variation of the COCO dataset and baseline BERT-CNN.</p>
          <p align="center"> <img src="BERT-CNN.png" width="300" align="middle" /></p>
          <a shape="rect" href="javascript:togglebib('ap_agent')" class="togglebib">Training data<sub>COCO</sub></a>
          <pre xml:space="preserve">
            visual from image, caption descriptions
            umbrella dress human face, a woman with an umbrella near the sea.
            bathtub tub,this is a bathroom with a jacuzzi shower sink and toilet.
            snowplow shovel,the fire hydrant is partially buried under the snow.
            desktop computer monitor,a computer with a flower as its background sits on a desk.
            pitcher ballplayer, a baseball player preparing to throw the ball.
            groom restaurant,a black and white picture of a centerpiece to a table at a wedding


          </pre>       
        </div>
      </td></tr>
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Overlaping<sub>COCO</sub> </a>
          <pre >
            visual from image, caption descriptions, overlapping information
            pole streetsign flagpole,a house that has a pole with a sign on it,{'pole'}
            stove microwave refrigerator,an older stove sits in the kitchen next to a bottle of cleaner,{'stove'}
            racket tennis ball ballplayer,a tennis player swinging a racket at a ball,{'tennis', 'racket', 'ball'}
            grocery store dining table restaurant,a table is full of different kinds of food and drinks,{'table'}
          </pre>       
        </div>

      </td></tr>
      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">Gender Natural<sub>COCO</sub> </a>
          <pre >
            visual from image, caption descriptions
            pizza, a person cutting a pizza with a fork and knife
            suit, a person in a suit and tie sitting with his hands between his legs.
            paddle, a person riding a colorful surfboard in the water.
           
          </pre>       
        </div>

      </td></tr>


      <tr><td width="1000" align="left"> 
        <div class="paper" id="ap_role_scenario1">
          <a shape="rect" href="javascript:togglebib('ap_role_scenario1')" class="togglebib">unsupervised<sub>CC</sub> </a>
          <pre xml:space="preserve">
            some tea in a wooden bowl with a scoop and blue flowers next to it
            two women standing side by side at an event holding their certificates
            an elderly man rides a bicycle in the street while people walk around
            the soccer player has his arms up as he is celebrating
            a stream runs through a forested green and leafy area
            two teams of men playing a game in a basketball court
            three ladies looking at something while sitting next to each other
            there is a shark swimming in the blue water
            
          </pre>       
        </div>
      </td></tr>

      
    </table> 
</div>

</br>

<div style="text-align: center;" class="containersmall">
  <span align="center"><a href="bibtex.txt">[bibtex]</a></span>
  <p>Contact: <a href="mailto:asabir@cs.upc.edu">Ahmed Sabir</a></p>
</div>
 
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

</body>
</html>
